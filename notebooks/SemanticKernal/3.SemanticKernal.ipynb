{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build a Retrieval Augmented Generation (RAG) Application with Semantic Kernel\n",
        "\n",
        "## Workshop Overview\n",
        "\n",
        "Welcome to this hands-on tutorial where you'll learn how to build a **Retrieval Augmented Generation (RAG)** application using **Microsoft Semantic Kernel**!\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "This notebook takes a progressive approach, building your understanding step-by-step:\n",
        "\n",
        "1. **Basic LLM Interaction** - Start with simple prompting using Semantic Kernel\n",
        "2. **Prompt Engineering** - Learn to structure prompts with SK templates\n",
        "3. **Function Calling** - Understand SK's plugin architecture\n",
        "4. **Document Loading & Splitting** - Prepare data for retrieval\n",
        "5. **Vector Embeddings & Storage** - Create searchable knowledge bases with InMemoryStore\n",
        "6. **Complete RAG Implementation** - Put it all together with SK functions\n",
        "\n",
        "### What is RAG?\n",
        "\n",
        "**Retrieval Augmented Generation (RAG)** is one of the most powerful applications enabled by LLMs. It allows AI to answer questions using **specific source information** rather than relying solely on its training data.\n",
        "\n",
        "Think of it like giving an AI assistant access to a specialized library - it can look up relevant information before answering your questions!\n",
        "\n",
        "### Why Semantic Kernel?\n",
        "\n",
        "**Semantic Kernel (SK)** is Microsoft's lightweight SDK for integrating AI services into applications. Key benefits:\n",
        "\n",
        "- ‚úÖ **Cross-platform** - Works with Python, C#, and Java\n",
        "- ‚úÖ **Plugin architecture** - Modular, reusable components\n",
        "- ‚úÖ **Enterprise-ready** - Built for production use\n",
        "- ‚úÖ **Multi-AI support** - Works with Azure OpenAI, OpenAI, and more\n",
        "- ‚úÖ **Function calling** - Natural integration with tools and data sources\n",
        "\n",
        "### The RAG Architecture\n",
        "\n",
        "A typical RAG application has **two main components**:\n",
        "\n",
        "1. **Indexing** (Offline) - Preparing your knowledge base\n",
        "   - Load documents from various sources\n",
        "   - Split large documents into chunks\n",
        "   - Create embeddings and store in a vector database\n",
        "\n",
        "2. **Retrieval & Generation** (Runtime) - Answering questions\n",
        "   - Retrieve relevant documents based on the query\n",
        "   - Generate answers using retrieved context\n",
        "\n",
        "---\n",
        "\n",
        "Let's get started! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Environment Setup\n",
        "\n",
        "Before we begin, let's install all the required packages for **Semantic Kernel**.\n",
        "\n",
        "### What We'll Install:\n",
        "\n",
        "- **semantic-kernel** - Microsoft's AI orchestration SDK\n",
        "- **python-dotenv** - Environment variable management\n",
        "- **faiss-cpu** - Facebook AI Similarity Search for vector storage\n",
        "- **beautifulsoup4** - HTML parsing for web content loading\n",
        "\n",
        "Run the cell below to install all dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "5n8AcmJ3BuPj",
        "outputId": "9edb9c2b-547c-44a4-9efc-f1b2473ac51c"
      },
      "outputs": [],
      "source": [
        "%pip install semantic-kernel python-dotenv\n",
        "%pip install faiss-cpu\n",
        "%pip install beautifulsoup4\n",
        "%pip install aiohttp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Basic LLM Interaction with Semantic Kernel\n",
        "\n",
        "Let's start with the fundamentals - setting up Semantic Kernel and making a simple request.\n",
        "\n",
        "### What's Happening Here?\n",
        "\n",
        "1. **Import Semantic Kernel** - Load the SK SDK\n",
        "2. **Load environment variables** - Read Azure OpenAI credentials from `.env` file\n",
        "3. **Create a Kernel** - The central orchestrator in SK\n",
        "4. **Add AI Service** - Connect to Azure OpenAI\n",
        "5. **Make a simple query** - Ask a question directly\n",
        "\n",
        "### Key Concept: The Kernel\n",
        "\n",
        "In Semantic Kernel, the **Kernel** is the central orchestrator that:\n",
        "- Manages AI services (chat, embeddings)\n",
        "- Hosts plugins and functions\n",
        "- Handles prompt execution\n",
        "- Coordinates function calling\n",
        "\n",
        "This is the **simplest** way to interact with an LLM using SK, but notice:\n",
        "- ‚ùå No specialized knowledge\n",
        "- ‚ùå No structured prompting\n",
        "- ‚ùå Limited control over format\n",
        "- ‚ùå Answers only from training data\n",
        "\n",
        "Let's see it in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Import Semantic Kernel components\n",
        "from semantic_kernel import Kernel\n",
        "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Create the Kernel - the central orchestrator\n",
        "kernel = Kernel()\n",
        "\n",
        "# Add Azure OpenAI Chat Completion service\n",
        "kernel.add_service(\n",
        "    AzureChatCompletion(\n",
        "        service_id=\"chat\",\n",
        "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "        endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "        deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"‚úì Semantic Kernel initialized with Azure OpenAI\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Basic LLM Call\n",
        "\n",
        "Let's ask the LLM a simple question using `invoke_prompt`. Notice how it answers from its general training knowledge:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DH1HpyEGCSj7",
        "outputId": "86555d84-b458-407a-f780-a8b39125b2a3"
      },
      "outputs": [],
      "source": [
        "# Simple prompt invocation\n",
        "answer = await kernel.invoke_prompt(\"What is machine learning?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYLbVxZICefk",
        "outputId": "a7eebcd1-b072-411d-fc17-9c10a59ebd70"
      },
      "outputs": [],
      "source": [
        "print(f\"Answer: {answer}\")\n",
        "print(f\"Type: {type(answer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Response\n",
        "\n",
        "Semantic Kernel's `invoke_prompt` returns a `FunctionResult` object containing:\n",
        "- **value** - The actual text answer from the LLM\n",
        "- **metadata** - Information about the execution\n",
        "\n",
        "The LLM gives a general answer based on its training, but it doesn't have access to:\n",
        "- ‚úó Up-to-date information\n",
        "- ‚úó Your company's internal documentation  \n",
        "- ‚úó Specific domain knowledge from your documents\n",
        "\n",
        "**This is where RAG becomes powerful!** But first, let's learn about prompt engineering with SK..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw74zandCtzm"
      },
      "source": [
        "## Step 2: Prompt Engineering with Semantic Kernel\n",
        "\n",
        "Now let's improve our LLM interactions using **SK's prompt template system**!\n",
        "\n",
        "### Why Use Prompt Templates in SK?\n",
        "\n",
        "Semantic Kernel provides powerful prompt engineering capabilities:\n",
        "\n",
        "- ‚úÖ **Variable Substitution** - Use `{{$variable}}` for dynamic content\n",
        "- ‚úÖ **Function Calling** - Embed function results with `{{plugin.function}}`\n",
        "- ‚úÖ **Multi-shot Prompting** - Structure conversations easily\n",
        "- ‚úÖ **Prompt Configuration** - Separate prompts from code\n",
        "- ‚úÖ **Reusability** - Create prompt libraries\n",
        "\n",
        "### SK Template Syntax\n",
        "\n",
        "Semantic Kernel uses a special syntax:\n",
        "- `{{$input}}` - Access input variables\n",
        "- `{{$variable_name}}` - Access named variables\n",
        "- `{{plugin.function}}` - Call SK functions\n",
        "- Multi-line templates with proper formatting\n",
        "\n",
        "This gives you:\n",
        "- **Consistency** - Same structure every time\n",
        "- **Reusability** - One template, many uses\n",
        "- **Maintainability** - Update once, affects all uses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeAs-D8-C1Qb"
      },
      "source": [
        "### Example 1: Simple Variable Substitution\n",
        "\n",
        "Let's create a template with variables using SK's `{{$variable}}` syntax:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sx3EH62vb9JU"
      },
      "outputs": [],
      "source": [
        "from semantic_kernel.functions import KernelArguments\n",
        "\n",
        "# SK template using {{$variable}} syntax\n",
        "simple_template = \"\"\"\n",
        "You are a helpful AI assistant named {{$bot_name}}.\n",
        "User question: {{$question}}\n",
        "\n",
        "Please provide a helpful answer.\n",
        "\"\"\"\n",
        "\n",
        "# Invoke with arguments\n",
        "result = await kernel.invoke_prompt(\n",
        "    function_name=\"simple_prompt\",\n",
        "    plugin_name=\"PromptDemo\",\n",
        "    prompt=simple_template,\n",
        "    arguments=KernelArguments(\n",
        "        bot_name=\"SKBot\",\n",
        "        question=\"What is Semantic Kernel?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how SK filled in the `{{$bot_name}}` and `{{$question}}` placeholders!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Structured Technical Writing Template\n",
        "\n",
        "Here's a more sophisticated template for a specific task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Technical documentation template\n",
        "tech_writer_template = \"\"\"\n",
        "You are a world-class technical documentation writer.\n",
        "\n",
        "Topic: {{$topic}}\n",
        "\n",
        "Please write a clear, concise explanation suitable for developers.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\"\"\"\n",
        "\n",
        "result = await kernel.invoke_prompt(\n",
        "    function_name=\"tech_writer\",\n",
        "    plugin_name=\"PromptDemo\",\n",
        "    prompt=tech_writer_template,\n",
        "    arguments=KernelArguments(topic=\"What is RAG (Retrieval Augmented Generation)?\")\n",
        ")\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: SK Plugins and Functions\n",
        "\n",
        "Now let's learn about **Semantic Kernel's plugin architecture** - a key differentiator from other frameworks!\n",
        "\n",
        "### What are SK Plugins?\n",
        "\n",
        "**Plugins** in Semantic Kernel are collections of functions that extend the kernel's capabilities:\n",
        "\n",
        "- ‚úÖ **Native Functions** - Python functions decorated with `@kernel_function`\n",
        "- ‚úÖ **Prompt Functions** - AI-powered functions defined as prompts\n",
        "- ‚úÖ **Reusable** - Share across projects\n",
        "- ‚úÖ **Composable** - Combine multiple functions\n",
        "- ‚úÖ **Auto-invokable** - LLM can call them automatically\n",
        "\n",
        "### Why is This Powerful?\n",
        "\n",
        "Unlike simple chaining, SK's plugin system allows:\n",
        "- **Modular Design** - Organize functions by domain\n",
        "- **Function Calling** - LLM automatically chooses which function to use\n",
        "- **State Management** - Functions can maintain state\n",
        "- **Tool Integration** - Connect to APIs, databases, etc.\n",
        "\n",
        "Let's create our first plugin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Create a Simple Native Plugin\n",
        "\n",
        "Let's create a plugin with a native Python function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantic_kernel.functions import kernel_function\n",
        "\n",
        "# Create a simple plugin class\n",
        "class TextPlugin:\n",
        "    @kernel_function(\n",
        "        name=\"get_word_count\",\n",
        "        description=\"Counts the number of words in a text\"\n",
        "    )\n",
        "    def get_word_count(self, text: str) -> str:\n",
        "        word_count = len(text.split())\n",
        "        return f\"The text contains {word_count} words.\"\n",
        "    \n",
        "    @kernel_function(\n",
        "        name=\"uppercase\",\n",
        "        description=\"Converts text to uppercase\"\n",
        "    )\n",
        "    def to_uppercase(self, text: str) -> str:\n",
        "        return text.upper()\n",
        "\n",
        "# Add the plugin to the kernel\n",
        "kernel.add_plugin(TextPlugin(), plugin_name=\"TextPlugin\")\n",
        "\n",
        "print(\"‚úì TextPlugin added to kernel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the Plugin Functions\n",
        "\n",
        "Now let's call our plugin functions directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Invoke plugin functions directly\n",
        "test_text = \"Semantic Kernel makes AI development easier and more organized\"\n",
        "\n",
        "word_count_result = await kernel.invoke(\n",
        "    function_name=\"get_word_count\",\n",
        "    plugin_name=\"TextPlugin\",\n",
        "    arguments=KernelArguments(text=test_text)\n",
        ")\n",
        "\n",
        "uppercase_result = await kernel.invoke(\n",
        "    function_name=\"uppercase\",\n",
        "    plugin_name=\"TextPlugin\",\n",
        "    arguments=KernelArguments(text=test_text)\n",
        ")\n",
        "\n",
        "print(f\"Word Count: {word_count_result}\")\n",
        "print(f\"Uppercase: {uppercase_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Document Loading & Text Splitting\n",
        "\n",
        "Now we get to the heart of RAG! Let's learn how to **load and split documents** for retrieval.\n",
        "\n",
        "### The RAG Indexing Pipeline\n",
        "\n",
        "Before we can retrieve, we need to **index our documents**:\n",
        "\n",
        "```\n",
        "Document ‚Üí Load ‚Üí Split ‚Üí Embed ‚Üí Store in Vector Database\n",
        "```\n",
        "\n",
        "This happens **offline** (once) to prepare your knowledge base.\n",
        "\n",
        "### What We'll Build\n",
        "\n",
        "1. **Load** document content (we'll use mock data for this tutorial)\n",
        "2. **Split** it into chunks using a text splitter\n",
        "3. **Embed** each chunk (convert to vectors) - Next step\n",
        "4. **Store** in a vector database using SK's InMemoryStore\n",
        "\n",
        "### Why Split Documents?\n",
        "\n",
        "- LLMs have limited context windows\n",
        "- Smaller chunks = more precise retrieval\n",
        "- Better matching between queries and relevant content\n",
        "- Easier to attribute sources\n",
        "\n",
        "Let's start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.1: Create Mock Documents\n",
        "\n",
        "For this tutorial, we'll use mock documents about AI topics (similar to the reference implementation):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mock paragraph data - sample documents about AI topics\n",
        "MOCK_DOCUMENTS = \"\"\"\n",
        "Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\n",
        "\n",
        "Machine learning is a subset of artificial intelligence that focuses on the development of algorithms and statistical models that enable computer systems to improve their performance on a specific task through experience. Deep learning, a subset of machine learning, uses neural networks with multiple layers to progressively extract higher-level features from raw input.\n",
        "\n",
        "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. NLP is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\n",
        "\n",
        "Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do. Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images.\n",
        "\n",
        "Reinforcement learning is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n",
        "\n",
        "Neural networks are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn to perform tasks by considering examples, generally without being programmed with task-specific rules. For instance, in image recognition, they might learn to identify images that contain cats by analyzing example images.\n",
        "\n",
        "Generative AI refers to artificial intelligence systems capable of generating text, images, or other media in response to prompts. Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics. Examples include large language models like GPT and image generation models like DALL-E.\n",
        "\n",
        "The Transformer architecture is a neural network architecture that has become the foundation for many modern AI models. It uses self-attention mechanisms to process input data in parallel, making it highly efficient for tasks like language translation and text generation. Transformers have revolutionized natural language processing since their introduction in 2017.\n",
        "\n",
        "Retrieval Augmented Generation (RAG) is a technique that enhances large language models by retrieving relevant information from external knowledge sources before generating responses. RAG combines the power of pre-trained language models with the ability to access up-to-date, domain-specific information, making AI responses more accurate and grounded in facts.\n",
        "\n",
        "Semantic Kernel is Microsoft's open-source SDK that helps developers integrate AI services into their applications. It provides a plugin-based architecture that makes it easy to combine multiple AI models, prompts, and native code into unified workflows. Semantic Kernel supports function calling, memory management, and orchestration of complex AI pipelines.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Loaded document with {len(MOCK_DOCUMENTS)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.2: Create a Text Splitter Function\n",
        "\n",
        "We need to split our document into manageable chunks. Let's create a simple paragraph-based splitter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_into_paragraphs(text: str) -> list[str]:\n",
        "    \"\"\"Split text into paragraphs by empty lines.\"\"\"\n",
        "    paragraphs = [p.strip() for p in text.strip().split('\\n\\n') if p.strip()]\n",
        "    return paragraphs\n",
        "\n",
        "# Split the documents\n",
        "paragraphs = split_into_paragraphs(MOCK_DOCUMENTS)\n",
        "print(f\"Split document into {len(paragraphs)} paragraphs\\n\")\n",
        "\n",
        "# Show first paragraph as example\n",
        "print(\"Example paragraph:\")\n",
        "print(f\"{paragraphs[0][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Vector Embeddings & InMemoryStore\n",
        "\n",
        "Now let's convert our text chunks into **vector embeddings** and store them in Semantic Kernel's **InMemoryStore**.\n",
        "\n",
        "### What are Vector Embeddings?\n",
        "\n",
        "Embeddings convert text into numerical vectors (lists of numbers) that capture semantic meaning:\n",
        "- Similar concepts have similar vectors\n",
        "- Enables semantic search (meaning-based, not keyword-based)\n",
        "- Typically 1536 dimensions for Azure OpenAI's text-embedding models\n",
        "\n",
        "### SK's InMemoryStore\n",
        "\n",
        "Semantic Kernel provides `InMemoryCollection` for vector storage:\n",
        "- ‚úÖ **Simple setup** - No external database needed\n",
        "- ‚úÖ **Built-in search** - Semantic similarity search included\n",
        "- ‚úÖ **Dataclass-based** - Type-safe document models\n",
        "- ‚úÖ **Auto-embedding** - Automatic vector generation\n",
        "\n",
        "Let's set it up!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5.1: Setup Azure Text Embedding Service\n",
        "\n",
        "First, let's add the embedding service to our kernel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantic_kernel.connectors.ai.open_ai import AzureTextEmbedding\n",
        "\n",
        "# Add embedding service to kernel\n",
        "text_embedding = AzureTextEmbedding(\n",
        "    service_id=\"embedding\",\n",
        "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    deployment_name=os.getenv(\"AZURE_OPENAI_ADA_DEPLOYMENT\")\n",
        ")\n",
        "\n",
        "kernel.add_service(text_embedding)\n",
        "print(\"‚úì Azure Text Embedding service added to kernel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5.2: Define Document Model with Vector Store Decorator\n",
        "\n",
        "SK uses dataclasses with special decorators to define document models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Annotated\n",
        "from semantic_kernel.data.vector import VectorStoreField, vectorstoremodel\n",
        "\n",
        "@vectorstoremodel(collection_name=\"documents\")\n",
        "@dataclass\n",
        "class DocumentParagraph:\n",
        "    \"\"\"Data model for storing document paragraphs with embeddings\"\"\"\n",
        "    id: Annotated[str, VectorStoreField(\"key\")]\n",
        "    text: Annotated[str, VectorStoreField(\"data\")]\n",
        "    embedding: Annotated[\n",
        "        list[float] | None,\n",
        "        VectorStoreField(\n",
        "            \"vector\",\n",
        "            dimensions=1536,\n",
        "            embedding_generator=text_embedding\n",
        "        ),\n",
        "    ] = None\n",
        "\n",
        "print(\"‚úì DocumentParagraph model defined\")\n",
        "print(\"  - Fields: id (key), text (data), embedding (vector)\")\n",
        "print(\"  - Auto-embedding enabled via text_embedding service\")\n",
        "print(\"  - Dimensions: 1536 (Azure OpenAI text-embedding-ada-002)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Complete RAG Implementation with Semantic Kernel\n",
        "\n",
        "Now let's put it all together! We'll:\n",
        "1. Create an InMemoryCollection\n",
        "2. Index our documents with embeddings\n",
        "3. Create a search function (recall)\n",
        "4. Build RAG queries\n",
        "\n",
        "### The Complete RAG Flow\n",
        "\n",
        "```\n",
        "User Question ‚Üí Search Vector Store ‚Üí Retrieve Relevant Docs ‚Üí Generate Answer with Context\n",
        "```\n",
        "\n",
        "This is where Semantic Kernel really shines with its **unified approach**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6.1: Create InMemoryCollection and Index Documents\n",
        "\n",
        "Let's create the collection, index our paragraphs, and set up the search function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantic_kernel.connectors.in_memory import InMemoryCollection\n",
        "\n",
        "# Create the in-memory collection\n",
        "collection = InMemoryCollection(record_type=DocumentParagraph)\n",
        "\n",
        "# Initialize the collection\n",
        "await collection.ensure_collection_exists()\n",
        "\n",
        "# Create DocumentParagraph objects and upsert into collection\n",
        "document_items = [\n",
        "    DocumentParagraph(id=f\"para_{i}\", text=para)\n",
        "    for i, para in enumerate(paragraphs)\n",
        "]\n",
        "\n",
        "await collection.upsert(document_items)\n",
        "\n",
        "print(f\"‚úì Loaded {len(paragraphs)} paragraphs into the vector store\")\n",
        "print(\"‚úì Documents successfully indexed with embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Understanding Vector Embeddings (Optional Demo)\n",
        "\n",
        "Before we move on to RAG queries, let's take a moment to understand what just happened with vector embeddings!\n",
        "\n",
        "**What are vectors?**\n",
        "- Each paragraph was converted into a list of 1,536 numbers\n",
        "- These numbers capture the *semantic meaning* of the text\n",
        "- Similar texts have similar vectors\n",
        "\n",
        "Let's see this in action with a simple similarity comparison:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec_a: list[float], vec_b: list[float]) -> float:\n",
        "    \"\"\"Calculate cosine similarity between two vectors (0-1, higher = more similar)\"\"\"\n",
        "    return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
        "\n",
        "# Test sentences with different similarity levels\n",
        "test_sentences = [\n",
        "    \"Machine learning uses algorithms to learn from data\",\n",
        "    \"Algorithms enable computers to learn from experience\",  # Similar meaning\n",
        "    \"Pizza and pasta are Italian foods\"  # Completely different topic\n",
        "]\n",
        "\n",
        "# Generate embeddings for test sentences\n",
        "print(\"Generating embeddings for test sentences...\\n\")\n",
        "test_embeddings = []\n",
        "for sentence in test_sentences:\n",
        "    response = await text_embedding.generate_embeddings([sentence])\n",
        "    test_embeddings.append(response[0])\n",
        "\n",
        "# Compare first sentence with the other two\n",
        "base_sentence = test_sentences[0]\n",
        "print(f\"Base sentence: '{base_sentence}'\\n\")\n",
        "print(\"Similarity comparisons:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i in range(1, len(test_sentences)):\n",
        "    similarity = cosine_similarity(test_embeddings[0], test_embeddings[i])\n",
        "    print(f\"üìä Comparison sentence: '{test_sentences[i]}'\")\n",
        "    print(f\"   Similarity score: {similarity:.4f}\")\n",
        "    print()\n",
        "\n",
        "print(\"üí° Notice: Similar meanings have higher scores (closer to 1.0)\")\n",
        "print(\"   Even when different words are used!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What Just Happened?**\n",
        "\n",
        "1. We converted three sentences into vectors (1,536 numbers each)\n",
        "2. We used **cosine similarity** to measure how \"close\" the meanings are\n",
        "3. The second sentence scores high (~0.85-0.95) because it means nearly the same thing\n",
        "4. The third sentence scores low (~0.60-0.75) because it's about a completely different topic\n",
        "\n",
        "**This is how vector search works:**\n",
        "- Your query gets converted to a vector\n",
        "- The system finds the most similar document vectors\n",
        "- Similar meaning = high similarity score = relevant results!\n",
        "\n",
        "Now let's test our actual RAG system with real queries! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6.2: Create Search Function (Recall)\n",
        "\n",
        "SK's InMemoryCollection can automatically create a search function that we add as a plugin:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a search function for the collection\n",
        "kernel.add_function(\n",
        "    \"memory\",\n",
        "    collection.create_search_function(\n",
        "        function_name=\"recall\",\n",
        "        description=\"Recalls information from the document collection about AI topics.\",\n",
        "        string_mapper=lambda x: x.record.text,  # Extract text from search results\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"‚úì Search function 'memory.recall' added to kernel\")\n",
        "print(\"  - Function automatically performs vector similarity search\")\n",
        "print(\"  - Returns relevant document chunks based on query\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6.3: Template-Based RAG Query\n",
        "\n",
        "Let's test our first RAG query using SK's template syntax with `{{memory.recall}}`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple RAG query using template syntax\n",
        "query = \"What are the main benefits of prompt engineering?\"\n",
        "\n",
        "rag_prompt = f\"\"\"\n",
        "Use the information below to answer the question.\n",
        "\n",
        "Context: {{{{memory.recall '{query}'}}}}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "result = await kernel.invoke_prompt(\n",
        "    function_name=\"rag_query\",\n",
        "    plugin_name=\"rag\",\n",
        "    prompt=rag_prompt\n",
        ")\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"\\nAnswer:\\n{result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What Just Happened?**\n",
        "\n",
        "When we invoked the prompt:\n",
        "1. SK saw `{{memory.recall 'query'}}` and executed the recall function\n",
        "2. The recall function searched the InMemoryCollection for relevant chunks\n",
        "3. Those chunks were injected into the Context section of the prompt\n",
        "4. The LLM used that context to answer the question\n",
        "\n",
        "Let's try another query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Another RAG query\n",
        "query2 = \"How do AI agents differ from traditional chatbots?\"\n",
        "\n",
        "rag_prompt2 = f\"\"\"\n",
        "Use the information below to answer the question.\n",
        "\n",
        "Context: {{{{memory.recall '{query2}'}}}}\n",
        "\n",
        "Question: {query2}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "result2 = await kernel.invoke_prompt(\n",
        "    function_name=\"rag_query2\",\n",
        "    plugin_name=\"rag\",\n",
        "    prompt=rag_prompt2\n",
        ")\n",
        "\n",
        "print(f\"Query: {query2}\")\n",
        "print(f\"\\nAnswer:\\n{result2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 7: Auto Function Calling (Advanced)\n",
        "\n",
        "**What We're About to Build:**\n",
        "\n",
        "So far we've explicitly called `{{memory.recall}}` in our prompts. But Semantic Kernel can automatically determine when to call functions based on the user's question!\n",
        "\n",
        "This is the foundation of **agentic behavior** - the LLM decides which tools to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7.1: Enable Auto Function Calling\n",
        "\n",
        "We'll configure the kernel to automatically call functions when needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
        "\n",
        "# Create execution settings with auto function calling enabled\n",
        "execution_settings = kernel.get_prompt_execution_settings_from_service_id(\"chat\")\n",
        "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
        "\n",
        "print(\"‚úÖ Auto function calling enabled!\")\n",
        "print(\"The LLM will now automatically decide when to call the memory.recall function.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7.2: Ask Questions Without Explicit Function Calls\n",
        "\n",
        "Now let's ask questions naturally - the LLM will automatically search the documents when needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask a question that requires document search\n",
        "user_question = \"What are the key capabilities of AI agents?\"\n",
        "\n",
        "# Simple prompt - no explicit function calls!\n",
        "simple_prompt = f\"Answer this question: {user_question}\"\n",
        "\n",
        "# The LLM will automatically call memory.recall if needed\n",
        "result = await kernel.invoke_prompt(\n",
        "    prompt=simple_prompt,\n",
        "    function_name=\"auto_rag\",\n",
        "    plugin_name=\"rag\",\n",
        "    arguments={\"settings\": execution_settings}\n",
        ")\n",
        "\n",
        "print(f\"Question: {user_question}\")\n",
        "print(f\"\\nAnswer:\\n{result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What Happened Behind the Scenes?**\n",
        "\n",
        "The LLM analyzed the question, realized it needed more information, automatically called `memory.recall(\"key capabilities of AI agents\")`, and then used the results to answer!\n",
        "\n",
        "Let's try more questions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test multiple questions with auto function calling\n",
        "questions = [\n",
        "    \"Explain vector embeddings in simple terms\",\n",
        "    \"What is the difference between RAG and fine-tuning?\",\n",
        "    \"How does prompt engineering improve AI responses?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = await kernel.invoke_prompt(\n",
        "        prompt=f\"Answer concisely: {question}\",\n",
        "        function_name=\"auto_rag_multi\",\n",
        "        plugin_name=\"rag\",\n",
        "        arguments={\"settings\": execution_settings}\n",
        "    )\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {result}\\n\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì Conclusion: What You've Learned\n",
        "\n",
        "Congratulations! You've built a complete RAG system with Semantic Kernel from the ground up.\n",
        "\n",
        "### Journey Recap:\n",
        "\n",
        "**Step 0: Environment Setup**\n",
        "- Installed Semantic Kernel and dependencies\n",
        "\n",
        "**Step 1: Basic LLM Interaction**\n",
        "- Created a kernel\n",
        "- Added Azure OpenAI chat service\n",
        "- Made your first AI call\n",
        "\n",
        "**Step 2: Prompt Engineering**\n",
        "- Used SK template syntax (`{{$variable}}`)\n",
        "- Created reusable prompt templates\n",
        "- Structured better prompts\n",
        "\n",
        "**Step 3: Plugins & Functions**\n",
        "- Built a TextPlugin with `@kernel_function`\n",
        "- Registered functions with the kernel\n",
        "- Called functions in prompts\n",
        "\n",
        "**Step 4: Document Processing**\n",
        "- Loaded and split documents into chunks\n",
        "- Prepared text for embeddings\n",
        "\n",
        "**Step 5: Vector Embeddings**\n",
        "- Set up Azure Text Embedding service\n",
        "- Created document models with `@vectorstoremodel`\n",
        "- Understood how semantic search works\n",
        "\n",
        "**Step 6: RAG Implementation**\n",
        "- Created InMemoryCollection\n",
        "- Indexed documents with embeddings\n",
        "- Built search functions\n",
        "- Performed template-based RAG queries\n",
        "\n",
        "**Step 7: Auto Function Calling**\n",
        "- Enabled `FunctionChoiceBehavior.Auto()`\n",
        "- Let the LLM decide when to search\n",
        "- Built true agentic behavior!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîë Key Semantic Kernel Concepts:\n",
        "\n",
        "| Concept | Purpose |\n",
        "|---------|---------|\n",
        "| **Kernel** | Central orchestrator managing services, plugins, and functions |\n",
        "| **Plugins** | Collections of functions organized by purpose |\n",
        "| **@kernel_function** | Decorator to expose Python functions to the kernel |\n",
        "| **Template Syntax** | `{{$variable}}` and `{{plugin.function}}` for dynamic prompts |\n",
        "| **InMemoryCollection** | Built-in vector store with automatic search |\n",
        "| **@vectorstoremodel** | Decorator to define document models for vector storage |\n",
        "| **FunctionChoiceBehavior** | Controls how the LLM selects and calls functions |\n",
        "\n",
        "### üÜö Semantic Kernel vs LangChain\n",
        "\n",
        "**Semantic Kernel:**\n",
        "- ‚úÖ Microsoft-native, tight Azure integration\n",
        "- ‚úÖ Plugin architecture with decorators\n",
        "- ‚úÖ Built-in vector stores\n",
        "- ‚úÖ Strong typing and async-first\n",
        "- ‚ö†Ô∏è Smaller community than LangChain\n",
        "\n",
        "**LangChain:**\n",
        "- ‚úÖ Larger ecosystem and community\n",
        "- ‚úÖ More integrations (100+ vector DBs, LLMs)\n",
        "- ‚úÖ Rich documentation and examples\n",
        "- ‚ö†Ô∏è Can be complex with many abstractions\n",
        "\n",
        "**Both are excellent choices!** Pick based on your ecosystem and team preferences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìö Resources & Next Steps\n",
        "\n",
        "**Official Documentation:**\n",
        "- [Semantic Kernel Docs](https://learn.microsoft.com/semantic-kernel/)\n",
        "- [Semantic Kernel GitHub](https://github.com/microsoft/semantic-kernel)\n",
        "- [Azure OpenAI Service Docs](https://learn.microsoft.com/azure/ai-services/openai/)\n",
        "\n",
        "**Advanced Topics to Explore:**\n",
        "1. **Planners** - Multi-step reasoning and planning\n",
        "2. **Memory Connectors** - Persistent storage (Redis, PostgreSQL, Qdrant)\n",
        "3. **Streaming** - Real-time response streaming\n",
        "4. **Multi-Agent Systems** - Coordinating multiple AI agents\n",
        "5. **Custom Connectors** - Integrate any LLM or service\n",
        "\n",
        "**Practice Exercises:**\n",
        "1. Add more documents to the collection\n",
        "2. Create a plugin with multiple related functions\n",
        "3. Build a chat interface that maintains conversation history\n",
        "4. Integrate a different vector database (Qdrant, Redis)\n",
        "5. Add function calling for non-RAG tasks (weather, calculations, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### üôè Thank You!\n",
        "\n",
        "You now have the foundation to build production-ready RAG systems with Semantic Kernel. Keep experimenting, and happy coding! üöÄ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
