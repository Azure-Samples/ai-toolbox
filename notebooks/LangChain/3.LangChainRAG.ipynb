{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build a Retrieval Augmented Generation (RAG) Application\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "This notebook takes a progressive approach, building your understanding step-by-step:\n",
        "\n",
        "1. **Basic LLM Interaction** - Start with simple prompting\n",
        "2. **Prompt Engineering** - Learn to structure prompts effectively\n",
        "3. **Chaining Components** - Connect multiple steps together\n",
        "4. **Document Loading & Splitting** - Prepare data for retrieval\n",
        "5. **Vector Embeddings & Storage** - Create searchable knowledge bases\n",
        "6. **Complete RAG Implementation** - Put it all together with LangGraph\n",
        "\n",
        "### What is RAG?\n",
        "\n",
        "**Retrieval Augmented Generation (RAG)** is one of the most powerful applications enabled by LLMs. It allows AI to answer questions using **specific source information** rather than relying solely on its training data.\n",
        "\n",
        "Think of it like giving an AI assistant access to a specialized library - it can look up relevant information before answering your questions!\n",
        "\n",
        "### The RAG Architecture\n",
        "\n",
        "A typical RAG application has **two main components**:\n",
        "\n",
        "1. **Indexing** (Offline) - Preparing your knowledge base\n",
        "   - Load documents from various sources\n",
        "   - Split large documents into chunks\n",
        "   - Create embeddings and store in a vector database\n",
        "\n",
        "2. **Retrieval & Generation** (Runtime) - Answering questions\n",
        "   - Retrieve relevant documents based on the query\n",
        "   - Generate answers using retrieved context\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's get started! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Environment Setup\n",
        "\n",
        "Before we begin, let's install all the required packages. We'll need:\n",
        "\n",
        "- **langchain** - The core framework for building LLM applications\n",
        "- **langchain-openai** - Azure OpenAI integration\n",
        "- **langchain-community** - Community-contributed components (document loaders, vector stores)\n",
        "- **langgraph** - Framework for building stateful, multi-step applications\n",
        "- **faiss-cpu** - Facebook AI Similarity Search for vector storage\n",
        "- **beautifulsoup4** - HTML parsing for web content loading\n",
        "\n",
        "Run the cell below to install all dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "5n8AcmJ3BuPj",
        "outputId": "9edb9c2b-547c-44a4-9efc-f1b2473ac51c"
      },
      "outputs": [],
      "source": [
        "%pip install langchain-text-splitters langchain-community langgraph\n",
        "%pip install langchain langchain-chroma langchain-openai\n",
        "%pip install -qU \"langchain[openai]\"\n",
        "%pip install faiss-cpu\n",
        "%pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Basic LLM Interaction\n",
        "\n",
        "Let's start with the fundamentals - connecting to an LLM and making a simple request.\n",
        "\n",
        "### What's Happening Here?\n",
        "\n",
        "1. **Load environment variables** - Read Azure OpenAI credentials from `.env` file\n",
        "2. **Create an LLM client** - Initialize connection to Azure OpenAI\n",
        "3. **Make a simple query** - Ask a question directly\n",
        "\n",
        "This is the **simplest** way to interact with an LLM, but notice:\n",
        "- ‚ùå No specialized knowledge\n",
        "- ‚ùå No structured prompting\n",
        "- ‚ùå Limited control over format\n",
        "- ‚ùå Answers only from training data\n",
        "\n",
        "Let's see it in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "# Use API key authentication for connection\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Basic LLM Call\n",
        "\n",
        "Let's ask the LLM a simple question. Notice how it answers from its general training knowledge:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DH1HpyEGCSj7",
        "outputId": "86555d84-b458-407a-f780-a8b39125b2a3"
      },
      "outputs": [],
      "source": [
        "answer = llm.invoke(\"how can langsmith help with testing?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYLbVxZICefk",
        "outputId": "a7eebcd1-b072-411d-fc17-9c10a59ebd70"
      },
      "outputs": [],
      "source": [
        "print(f\"answer: {answer}\")\n",
        "print(f\"Type: {type(answer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Response\n",
        "\n",
        "The LLM returns a structured response object containing:\n",
        "- **content** - The actual text answer\n",
        "- **response_metadata** - Technical details (tokens used, model info, etc.)\n",
        "- **type** - The object type (AIMessage)\n",
        "\n",
        "The LLM gives a general answer based on its training, but it doesn't have access to:\n",
        "- ‚úó Up-to-date information\n",
        "- ‚úó Your company's internal documentation  \n",
        "- ‚úó Specific domain knowledge from your documents\n",
        "\n",
        "**This is where RAG becomes powerful!** But first, let's learn about better prompting..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw74zandCtzm"
      },
      "source": [
        "## Step 2: Prompt Engineering with Templates\n",
        "\n",
        "Now let's improve our LLM interactions using **prompt templates**!\n",
        "\n",
        "### Why Use Prompt Templates?\n",
        "\n",
        "Direct LLM calls are limited. LangChain's power comes from **chaining components** together:\n",
        "\n",
        "- ‚úÖ **Prompt Templates** - Structure conversations effectively\n",
        "- ‚úÖ **Output Parsers** - Format responses consistently\n",
        "- ‚úÖ **Retrieval Components** - Add external knowledge\n",
        "- ‚úÖ **Multiple Processing Steps** - Build complex workflows\n",
        "- ‚úÖ **Data Transformations** - Clean and prepare data\n",
        "\n",
        "### What is a Prompt Template?\n",
        "\n",
        "Think of it as a **Mad Libs for AI**:\n",
        "- You create a template with placeholders (e.g., `{name}`, `{user_input}`)\n",
        "- You fill in the blanks with actual values\n",
        "- The completed prompt goes to the LLM\n",
        "\n",
        "This gives you:\n",
        "- **Consistency** - Same structure every time\n",
        "- **Reusability** - One template, many uses\n",
        "- **Maintainability** - Update once, affects all uses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeAs-D8-C1Qb"
      },
      "source": [
        "### Example 1: Multi-Turn Conversation Template\n",
        "\n",
        "Let's create a template that simulates a multi-turn conversation:\n",
        "\n",
        "**What's happening:**\n",
        "\n",
        "1. **Import ChatPromptTemplate** - LangChain's tool for structured conversations\n",
        "2. **Create conversation with roles** - Each message has a role:\n",
        "   - `\"system\"` - Sets the AI's identity and behavior (with placeholder `{name}`)\n",
        "   - `\"human\"` - User messages\n",
        "   - `\"ai\"` - Assistant responses\n",
        "3. **Use placeholders** - `{name}` and `{user_input}` will be filled in later\n",
        "4. **Invoke the template** - Fill the placeholders with actual values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sx3EH62vb9JU"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "    (\"human\", \"Hello, how are you doing?\"),\n",
        "    (\"ai\", \"I'm doing well, thanks!\"),\n",
        "    (\"human\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "prompt_value = template.invoke(\n",
        "    {\n",
        "        \"name\": \"Bob\",\n",
        "        \"user_input\": \"What is your name?\"\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdAKONbfcIUB",
        "outputId": "a92788f2-f08a-4ead-d7a3-fbc901e45c36"
      },
      "outputs": [],
      "source": [
        "for msg in prompt_value.messages:\n",
        "  print(type(msg).__name__, \":\", msg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See how the template filled in the placeholders? The conversation now has:\n",
        "- System message with \"Bob\" as the name\n",
        "- Human and AI exchange establishing context\n",
        "- Final human question asking for the name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZg13hhlC4GU"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a world class technical documentation writer.\"), # system instructions\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Simple Two-Message Template\n",
        "\n",
        "Here's a simpler pattern - just system instructions and user input:\n",
        "\n",
        "This is the most common RAG pattern:\n",
        "- Set the role/behavior in the system message\n",
        "- Take user input dynamically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMXV7nTUDGws",
        "outputId": "091fabdf-4926-4fc5-9a0b-b2c16c92ff54"
      },
      "outputs": [],
      "source": [
        "for msg in prompt.messages:\n",
        "  print(type(msg).__name__, \":\", msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice: The template has placeholders but isn't invoked yet, so `{input}` is still a variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Chaining Components Together\n",
        "\n",
        "Now for the magic! Let's **chain** our components using the pipe (`|`) operator.\n",
        "\n",
        "![chaining.png](../Assets/images/chaining.png)\n",
        "\n",
        "### What is Chaining?\n",
        "\n",
        "Think of it as a **pipeline** or **assembly line** where data flows through multiple steps:\n",
        "\n",
        "```\n",
        "Input ‚Üí Step 1 ‚Üí Step 2 ‚Üí Step 3 ‚Üí Output\n",
        "```\n",
        "\n",
        "The `|` symbol means \"**then**\" or \"**pipe to**\":\n",
        "\n",
        "```python\n",
        "chain = prompt | llm | output_parser\n",
        "```\n",
        "\n",
        "Translates to:\n",
        "1. Take the `prompt` template \n",
        "2. **THEN** send it to the `llm`\n",
        "3. **THEN** parse the output\n",
        "\n",
        "### Why is This Powerful?\n",
        "\n",
        "- ‚úÖ **Modular** - Swap components easily\n",
        "- ‚úÖ **Readable** - Clear flow of data\n",
        "- ‚úÖ **Reusable** - Components work anywhere\n",
        "- ‚úÖ **Testable** - Test each step independently"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTgC9sHCDLho"
      },
      "source": [
        "### Create a Simple Chain (2 Steps)\n",
        "\n",
        "Let's start with a basic 2-step chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This chain has 2 components:\n",
        "1. **prompt** - Formats the input\n",
        "2. **llm** - Generates the response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Think of it like a pipeline or assembly line:\n",
        "\n",
        "The | symbol = \"then\" or \"pipe to\"\n",
        "prompt = Your formatted question/template\n",
        "llm = The AI model that generates answers\n",
        "So chain = prompt | llm means:\n",
        "\n",
        "Take the prompt ‚Üí THEN ‚Üí send it to the LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIUx0q9lDBUI"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm\n",
        "## pass the prompt to the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inspecting Chain Components\n",
        "\n",
        "You can access different parts of the chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHye3USTDFAp",
        "outputId": "bab8bdab-e9ed-4c22-e479-b2abff62323a"
      },
      "outputs": [],
      "source": [
        "#chain.first shows you the first component in your chain.\n",
        "\n",
        "print(chain.middle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tFXsYndvDP2m",
        "outputId": "d031d8d8-4c46-4559-eec3-b326b70e2697"
      },
      "outputs": [],
      "source": [
        "chain_result = chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the Chain\n",
        "\n",
        "Now let's execute our 2-step chain:\n",
        "\n",
        "The chain processes:\n",
        "1. **prompt** fills `{input}` with our question\n",
        "2. **llm** generates a response based on the formatted prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpA2sJpm093e"
      },
      "source": [
        "\"system\": \"You are a world class technical documentation writer.\"\n",
        "\n",
        "\"user\", \"how can langsmith help with testing?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What actually got sent to the LLM was:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUU5LibFDT55",
        "outputId": "508f772e-9909-45ac-dc60-992d7b935844"
      },
      "outputs": [],
      "source": [
        "print(chain_result.content) #This line displays the actual text response from the AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This extracts just the text content. But the response object contains more:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View Response Metadata\n",
        "\n",
        "This displays technical information about the AI's response - all the \"behind the scenes\" details like token counts and model info:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awgcpboCdu2q",
        "outputId": "60a5a29f-2930-412d-ec73-fc966e65cf1a"
      },
      "outputs": [],
      "source": [
        "print(chain_result.response_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqZ1ud0dDWM3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding an Output Parser (3-Step Chain)\n",
        "\n",
        "The LLM returns a complex `AIMessage` object. Often we just want simple text!\n",
        "\n",
        "**StrOutputParser** extracts just the string content, making responses easier to work with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a 3-Step Chain\n",
        "\n",
        "Now we'll add the output parser to create a complete 3-step pipeline:\n",
        "\n",
        "**The 3 Steps:**\n",
        "1. **prompt** ‚Üí Format the question\n",
        "2. **llm** ‚Üí Get AI response (returns complex AIMessage object)\n",
        "3. **output_parser** ‚Üí Extract clean text string\n",
        "\n",
        "**Think of it like a car wash:**\n",
        "- Step 1: Prep the car (format prompt)\n",
        "- Step 2: Wash the car (get AI response)\n",
        "- Step 3: Dry and polish (clean up the output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkbmUs3pDdU9"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lh_F1FizDgPa",
        "outputId": "f7505070-a0e6-412e-e1ae-313de3926139"
      },
      "outputs": [],
      "source": [
        "chain_result = chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the 3-step chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAkrcwODDkPJ",
        "outputId": "47423c90-b867-4c2e-a111-c421c8f3f501"
      },
      "outputs": [],
      "source": [
        "print(chain_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice: Now we get a clean string directly, not an AIMessage object! Much easier to work with.\n",
        "\n",
        "**Key Insight:** We're still missing the \"R\" in RAG - **Retrieval**! The LLM is answering from its training data, not from specific documents we provide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_f5Dd3nFr86"
      },
      "source": [
        "## Step 4: Adding Retrieval - The \"R\" in RAG\n",
        "\n",
        "Now we get to the heart of RAG! Let's add the ability to **retrieve information from documents**.\n",
        "\n",
        "### The RAG Indexing Pipeline\n",
        "\n",
        "Before we can retrieve, we need to **index our documents**:\n",
        "\n",
        "```\n",
        "Document ‚Üí Load ‚Üí Split ‚Üí Embed ‚Üí Store in Vector Database\n",
        "```\n",
        "\n",
        "This happens **offline** (once) to prepare your knowledge base.\n",
        "\n",
        "### What We'll Build\n",
        "\n",
        "1. **Load** a document (web page)\n",
        "2. **Split** it into chunks\n",
        "3. **Embed** each chunk (convert to vectors)\n",
        "4. **Store** in a vector database (FAISS)\n",
        "5. **Retrieve** relevant chunks when asked\n",
        "\n",
        "Let's start!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rnaYZlqzGFU_",
        "outputId": "7e6650dd-d9ab-4460-c81d-d326e2ac4289"
      },
      "outputs": [],
      "source": [
        "#Imports a tool that can read websites\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
        "\n",
        "docs = loader.load() # The docs variable now contains all that website content, ready to be processed further!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.1: Load Documents\n",
        "\n",
        "First, we need source material. We'll use **WebBaseLoader** to load content from a website.\n",
        "\n",
        "**What's happening:**\n",
        "- `WebBaseLoader` fetches HTML from the URL\n",
        "- Parses it into plain text\n",
        "- Creates `Document` objects we can process\n",
        "\n",
        "The `docs` variable now contains all that website content, ready for the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.2: Split Documents & Create Vector Store\n",
        "\n",
        "Web pages are often too long for LLM context windows. We need to:\n",
        "1. **Split** the document into smaller chunks\n",
        "2. **Embed** each chunk (convert to vectors)\n",
        "3. **Store** in a searchable vector database\n",
        "\n",
        "**Think of it like creating a smart library:**\n",
        "\n",
        "- **Text Splitter** - Takes a huge book and breaks it into individual pages/chapters\n",
        "- **Embeddings** - Creates a \"topic fingerprint\" for each page describing its meaning\n",
        "- **FAISS** - Organizes all those fingerprints so you can quickly find relevant pages\n",
        "\n",
        "### Why Split?\n",
        "- LLMs have limited context windows\n",
        "- Smaller chunks = more precise retrieval\n",
        "- Better matching between queries and relevant content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCePZEL0Gj0G"
      },
      "outputs": [],
      "source": [
        "# First, set up the embeddings model\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    azure_deployment=os.environ[\"AZURE_OPENAI_ADA_DEPLOYMENT\"],\n",
        "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
        ")\n",
        "\n",
        "# Now split the documents and create vector store\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)\n",
        "vector = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "print(f\"‚úì Created vector store with {len(documents)} document chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see the document has been split into manageable chunks, each ready to be embedded and stored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Building the RAG Chain\n",
        "\n",
        "Now let's connect retrieval with generation! We'll create a chain that:\n",
        "1. Takes a question\n",
        "2. Retrieves relevant documents\n",
        "3. Uses those documents to answer\n",
        "\n",
        "### The Document Chain\n",
        "\n",
        "This creates a specialized chain for answering questions using provided documents as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sb63NBGGwuy"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "# create_stuff_documents_chain :\n",
        "#  Create a chain for passing a list of Documents to a model.\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\", output_parser = output_parser)\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "# document_chain = prompt | llm /"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key components:**\n",
        "- **Prompt with context** - Includes `{context}` placeholder for retrieved documents\n",
        "- **Instruction** - \"Answer based ONLY on the provided context\"\n",
        "- **document_chain** - Combines prompt and LLM to process documents\n",
        "\n",
        "This is the \"generation\" part of RAG!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complete the RAG Chain\n",
        "\n",
        "Now let's add the retrieval component:\n",
        "\n",
        "**What's happening:**\n",
        "- `retriever` - Searches the vector store for relevant documents\n",
        "- `retrieval_chain` - Combines retrieval + generation\n",
        "\n",
        "**Think of it like a research assistant:**\n",
        "- **Before RAG:** AI answers from general knowledge (may hallucinate)\n",
        "- **With RAG:** AI gets specific research papers and answers ONLY using those papers (grounded in facts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPdi0y7ZG29G"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = vector.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the Complete RAG System!\n",
        "\n",
        "This is the moment we've been building toward! Let's run the complete RAG pipeline:\n",
        "\n",
        "**The full flow:**\n",
        "1. User asks: \"how can langsmith help with testing?\"\n",
        "2. **Retrieval:** Search vector store for relevant document chunks\n",
        "3. **Generation:** LLM answers using ONLY the retrieved context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fchYCGjKHDKu",
        "outputId": "cad3a6a8-d971-433c-8a6b-48f12cb2a28b"
      },
      "outputs": [],
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmP7v_s7HHmv",
        "outputId": "e06297e0-8ff8-4fa3-a02c-c163487b44fe"
      },
      "outputs": [],
      "source": [
        "print(response[\"answer\"])\n",
        "\n",
        "# LangSmith offers several features that can help with testing:..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üéâ **Success!** The answer comes from the specific documentation we loaded, not generic training data!\n",
        "\n",
        "Notice how the answer is:\n",
        "- ‚úÖ Specific to LangSmith\n",
        "- ‚úÖ Based on retrieved documentation\n",
        "- ‚úÖ Grounded in factual content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYwphfNwYefc",
        "outputId": "e51ad310-453a-4f21-e658-b6361a554aa0"
      },
      "outputs": [],
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"how can use it?\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test with Another Question\n",
        "\n",
        "Let's try a different question to see RAG in action:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 6: Advanced RAG with LangGraph\n",
        "\n",
        "Now let's rebuild our RAG application using **LangGraph** - a framework for building **stateful, multi-step applications**.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "The simple chain we built works, but LangGraph adds:\n",
        "- ‚úÖ **State management** - Track conversation context\n",
        "- ‚úÖ **Multiple invocation modes** - Sync, async, streaming\n",
        "- ‚úÖ **Easier debugging** - Visualize application flow\n",
        "- ‚úÖ **Streamlined deployment** - Production-ready patterns\n",
        "- ‚úÖ **Better observability** - Built-in tracing\n",
        "\n",
        "### LangGraph Components\n",
        "\n",
        "To build a LangGraph application, we define:\n",
        "1. **State** - What data flows through the application\n",
        "2. **Nodes** - Individual processing steps\n",
        "3. **Control Flow** - How steps connect together\n",
        "\n",
        "Let's rebuild our RAG app with LangGraph!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load and Index a New Document\n",
        "\n",
        "For this advanced example, let's load a different document - a blog post about LLM agents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "import faiss\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Load and chunk contents of the blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # chunk size (characters)\n",
        "    chunk_overlap=200,  # chunk overlap (characters)\n",
        "    add_start_index=True,  # track index in original document\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Loaded {len(docs)} document(s)\")\n",
        "print(f\"Split into {len(all_splits)} chunks\")\n",
        "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
        "\n",
        "# Create a new vector store for this example\n",
        "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "vector_store_graph = FAISS(\n",
        "    embedding_function=embeddings,\n",
        "    index=index,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={},\n",
        ")\n",
        "\n",
        "# Index chunks\n",
        "_ = vector_store_graph.add_documents(documents=all_splits)\n",
        "print(\"‚úì Documents indexed in vector store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Application State\n",
        "\n",
        "State controls what data flows through our application. For RAG, we need:\n",
        "- **question** - User's query\n",
        "- **context** - Retrieved documents\n",
        "- **answer** - Generated response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "print(\"‚úì State defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Nodes (Application Steps)\n",
        "\n",
        "Nodes are individual functions that process the state. We need two:\n",
        "1. **retrieve** - Find relevant documents\n",
        "2. **generate** - Create answer from documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the RAG prompt from LangChain Hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Define application steps\n",
        "def retrieve(state: State):\n",
        "    \"\"\"Retrieve relevant documents based on the question\"\"\"\n",
        "    retrieved_docs = vector_store_graph.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    \"\"\"Generate answer using retrieved context\"\"\"\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "print(\"‚úì Nodes defined: retrieve, generate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build and Compile the Graph\n",
        "\n",
        "Now let's connect our nodes into a graph:\n",
        "\n",
        "**What's happening:**\n",
        "- `StateGraph(State)` - Create graph with our state type\n",
        "- `.add_sequence([retrieve, generate])` - Connect nodes in order\n",
        "- `.add_edge(START, \"retrieve\")` - Define entry point\n",
        "- `.compile()` - Finalize the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile application\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "print(\"‚úì Graph compiled successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize the Graph (Optional)\n",
        "\n",
        "LangGraph provides visualization to understand the flow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "    print(\"Note: Graph visualization requires additional dependencies\")\n",
        "    print(\"The graph still works without visualization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the LangGraph RAG Application\n",
        "\n",
        "Let's test our new application with a question about the blog post:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "print(f\"Answer: {response['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stream Results (See Steps in Real-Time)\n",
        "\n",
        "One of LangGraph's powerful features is streaming - watch each step execute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for step in graph.stream(\n",
        "    {\"question\": \"What is Task Decomposition?\"}, \n",
        "    stream_mode=\"updates\"\n",
        "):\n",
        "    print(f\"{step}\\n\\n{'='*50}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See how you can watch each step execute? First retrieval, then generation!\n",
        "\n",
        "### Stream Tokens (Real-Time Generation)\n",
        "\n",
        "You can even stream individual tokens as they're generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Streaming answer: \", end=\"\")\n",
        "for message, metadata in graph.stream(\n",
        "    {\"question\": \"What is Task Decomposition?\"}, \n",
        "    stream_mode=\"messages\"\n",
        "):\n",
        "    print(message.content, end=\"\", flush=True)\n",
        "print(\"\\n\\n‚úì Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Congratulations! üéâ\n",
        "\n",
        "You've successfully built a complete RAG application from scratch!\n",
        "\n",
        "### What You've Learned\n",
        "\n",
        "Let's recap the journey:\n",
        "\n",
        "1. **Basic LLM Interaction** ‚úì\n",
        "   - Connected to Azure OpenAI\n",
        "   - Made simple queries\n",
        "   - Understood response objects\n",
        "\n",
        "2. **Prompt Engineering** ‚úì\n",
        "   - Created prompt templates\n",
        "   - Structured conversations with roles\n",
        "   - Used placeholders for dynamic content\n",
        "\n",
        "3. **Chaining Components** ‚úì\n",
        "   - Connected prompt ‚Üí LLM ‚Üí parser\n",
        "   - Understood the pipe (`|`) operator\n",
        "   - Built modular, reusable components\n",
        "\n",
        "4. **Document Loading & Splitting** ‚úì\n",
        "   - Loaded web content\n",
        "   - Split into manageable chunks\n",
        "   - Prepared data for retrieval\n",
        "\n",
        "5. **Vector Embeddings & Storage** ‚úì\n",
        "   - Created embeddings with Azure OpenAI\n",
        "   - Stored in FAISS vector database\n",
        "   - Built searchable knowledge bases\n",
        "\n",
        "6. **Complete RAG Implementation** ‚úì\n",
        "   - Combined retrieval + generation\n",
        "   - Grounded answers in specific documents\n",
        "   - Reduced hallucinations\n",
        "\n",
        "7. **Advanced LangGraph** ‚úì\n",
        "   - Built stateful applications\n",
        "   - Defined nodes and control flow\n",
        "   - Enabled streaming and observability\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "**RAG Benefits:**\n",
        "- ‚úÖ **Up-to-date information** - Use current documents, not just training data\n",
        "- ‚úÖ **Domain-specific knowledge** - Incorporate your specialized content\n",
        "- ‚úÖ **Reduced hallucinations** - Answers grounded in real documents\n",
        "- ‚úÖ **Source attribution** - Track where answers come from\n",
        "- ‚úÖ **Easy updates** - Change knowledge base without retraining\n",
        "\n",
        "**RAG Architecture:**\n",
        "```\n",
        "Indexing (Offline):\n",
        "  Document ‚Üí Load ‚Üí Split ‚Üí Embed ‚Üí Vector Store\n",
        "\n",
        "Retrieval & Generation (Runtime):\n",
        "  Query ‚Üí Retrieve Relevant Docs ‚Üí Generate Answer with Context\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "To extend this RAG application further:\n",
        "\n",
        "1. **Add Conversation Memory** - Track chat history for multi-turn conversations\n",
        "2. **Implement Query Analysis** - Optimize search queries before retrieval\n",
        "3. **Add Metadata Filtering** - Filter documents by date, section, or category\n",
        "4. **Use Multiple Retrievers** - Combine different search strategies\n",
        "5. **Add Re-ranking** - Improve retrieval quality with re-ranking models\n",
        "6. **Deploy to Production** - Use LangGraph Platform for deployment\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [LangChain Documentation](https://python.langchain.com/)\n",
        "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
        "- [RAG Tutorial Part 2](https://python.langchain.com/docs/tutorials/rag/#next-steps) - Multi-turn conversations\n",
        "- [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/)\n",
        "- [LangSmith](https://smith.langchain.com/) - Tracing and debugging\n",
        "\n",
        "### Practice Exercise\n",
        "\n",
        "Try building your own RAG application:\n",
        "1. Choose a different document source (PDF, database, API)\n",
        "2. Experiment with different chunk sizes\n",
        "3. Try different embedding models\n",
        "4. Add custom prompts for your use case\n",
        "5. Implement conversation memory\n",
        "\n",
        "**Happy building!** üöÄ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
