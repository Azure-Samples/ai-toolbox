{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://python.langchain.com/docs/tutorials/rag/\n",
        "\n",
        "Build a Retrieval Augmented Generation (RAG) App: Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "5n8AcmJ3BuPj",
        "outputId": "9edb9c2b-547c-44a4-9efc-f1b2473ac51c"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-chroma langchain-openai\n",
        "!pip install beautifulsoup4\n",
        "!pip install langchain-community langchain-text-splitters langgraph\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIdQk4LZBx6t",
        "outputId": "b63060e8-abb5-483d-cec7-0e96af878ecc"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"LANGCHAIN_TRACING_V2: {LANGCHAIN_TRACING_V2}\")\n",
        "print(f\"OPENAI_API_KEY: {os.environ.get('OPENAI_API_KEY', 'Not Set')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pilOtfFm0qvU"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmvHHBCDCJ4f"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DH1HpyEGCSj7",
        "outputId": "86555d84-b458-407a-f780-a8b39125b2a3"
      },
      "outputs": [],
      "source": [
        "answer = llm.invoke(\"how can langsmith help with testing?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYLbVxZICefk",
        "outputId": "a7eebcd1-b072-411d-fc17-9c10a59ebd70"
      },
      "outputs": [],
      "source": [
        "print(f\"answer: {answer}\")\n",
        "print(f\"Type: {type(answer)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "# Load and chunk contents of the blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Index chunks\n",
        "_ = vector_store.add_documents(documents=all_splits)\n",
        "\n",
        "# Define prompt for question-answering\n",
        "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
        "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "\n",
        "# Define application steps\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "\n",
        "# Compile application and test\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw74zandCtzm"
      },
      "source": [
        "# Let's Do Some Chaining\n",
        "- This is the key point! LangChain's power comes from chaining different components together. In the simple example above, you're just making a direct call to the LLM. There's no:\n",
        "    - Prompt templates\n",
        "    - Output parsers\n",
        "    - Retrieval components\n",
        "    - Multiple processing steps\n",
        "    - Data transformations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeAs-D8-C1Qb"
      },
      "source": [
        "## Templating \n",
        "\n",
        "What it's doing:  \n",
        "\n",
        "1. Import the ChatPromptTemplate: This is LangChain's tool for creating structured conversation templates.\n",
        "\n",
        "2. Create a conversation template: The ChatPromptTemplate.from_messages() creates a template that represents a multi-turn conversation with different roles:\n",
        "\n",
        "```\n",
        "\"system\": Sets up the AI's identity and behavior (with a placeholder {name})  \n",
        "\"human\": First human message  \n",
        "\"ai\": AI's response to establish context  \n",
        "\"human\": Second human message (with a placeholder {user_input})  \n",
        "```\n",
        "\n",
        "3. Use placeholders: The curly braces {name} and {user_input} are variables that will be filled in later.\n",
        "\n",
        "\n",
        "4. Invoke the template: When you call template.invoke() with the dictionary of values:\n",
        "\n",
        "- {name} gets replaced with \"Bob\"\n",
        "- {user_input} gets replaced with \"What is your name?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sx3EH62vb9JU"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "    (\"human\", \"Hello, how are you doing?\"),\n",
        "    (\"ai\", \"I'm doing well, thanks!\"),\n",
        "    (\"human\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "prompt_value = template.invoke(\n",
        "    {\n",
        "        \"name\": \"Bob\",\n",
        "        \"user_input\": \"What is your name?\"\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdAKONbfcIUB",
        "outputId": "a92788f2-f08a-4ead-d7a3-fbc901e45c36"
      },
      "outputs": [],
      "source": [
        "for msg in prompt_value.messages:\n",
        "  print(type(msg).__name__, \":\", msg.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZg13hhlC4GU"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a world class technical documentation writer.\"), # system instructions\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMXV7nTUDGws",
        "outputId": "091fabdf-4926-4fc5-9a0b-b2c16c92ff54"
      },
      "outputs": [],
      "source": [
        "for msg in prompt.messages:\n",
        "  print(type(msg).__name__, \":\", msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![chaining.png](../Assets/images/chaining.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTgC9sHCDLho"
      },
      "source": [
        "## Chaining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Think of it like a pipeline or assembly line:\n",
        "\n",
        "The | symbol = \"then\" or \"pipe to\"\n",
        "prompt = Your formatted question/template\n",
        "llm = The AI model that generates answers\n",
        "So chain = prompt | llm means:\n",
        "\n",
        "Take the prompt → THEN → send it to the LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIUx0q9lDBUI"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm\n",
        "## pass the prompt to the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHye3USTDFAp",
        "outputId": "bab8bdab-e9ed-4c22-e479-b2abff62323a"
      },
      "outputs": [],
      "source": [
        "#chain.first shows you the first component in your chain.\n",
        "\n",
        "print(chain.middle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tFXsYndvDP2m",
        "outputId": "d031d8d8-4c46-4559-eec3-b326b70e2697"
      },
      "outputs": [],
      "source": [
        "chain_result = chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpA2sJpm093e"
      },
      "source": [
        "\"system\": \"You are a world class technical documentation writer.\"\n",
        "\n",
        "\"user\", \"how can langsmith help with testing?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUU5LibFDT55",
        "outputId": "508f772e-9909-45ac-dc60-992d7b935844"
      },
      "outputs": [],
      "source": [
        "print(chain_result.content) #This line displays the actual text response from the AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This displays the technical information about the AI's response - all the \"behind the scenes\" details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awgcpboCdu2q",
        "outputId": "60a5a29f-2930-412d-ec73-fc966e65cf1a"
      },
      "outputs": [],
      "source": [
        "print(chain_result.response_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqZ1ud0dDWM3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This creates a 3-step chain instead of the previous 2-step chain. Now you have:\n",
        "\n",
        "\n",
        "### The 3 Steps:\n",
        "- prompt → Format the question\n",
        "- llm → Get AI response (complex object)\n",
        "- output_parser → Clean up the response (convert to simple string)\n",
        "\n",
        "### Think of it like a car wash:\n",
        "\n",
        "- Step 1: Prep the car (format prompt)\n",
        "- Step 2: Wash the car (get AI response)\n",
        "- Step 3: Dry and polish (clean up the text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkbmUs3pDdU9"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lh_F1FizDgPa",
        "outputId": "f7505070-a0e6-412e-e1ae-313de3926139"
      },
      "outputs": [],
      "source": [
        "chain_result = chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAkrcwODDkPJ",
        "outputId": "47423c90-b867-4c2e-a111-c421c8f3f501"
      },
      "outputs": [],
      "source": [
        "print(chain_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_f5Dd3nFr86"
      },
      "source": [
        "# Where is the retrieval from <u>**R**</u>AG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rnaYZlqzGFU_",
        "outputId": "7e6650dd-d9ab-4460-c81d-d326e2ac4289"
      },
      "outputs": [],
      "source": [
        "#Imports a tool that can read websites\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
        "\n",
        "docs = loader.load() # The docs variable now contains all that website content, ready to be processed further!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYOb56WDGYuG"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings() # Creates an instance of the embeddings model that's ready to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Take Langchains website content and create a searchable database of it.\n",
        "\n",
        "Think of it like creating a smart library:\n",
        "\n",
        "- Text Splitter: Takes a huge book and breaks it into individual pages/chapters\n",
        "- Embeddings: Creates a \"topic card\" for each page that describes what it's about\n",
        "- FAISS: Organizes all those topic cards so you can quickly find relevant pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCePZEL0Gj0G"
      },
      "outputs": [],
      "source": [
        "#This code takes your website content and creates a searchable database of it.\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)\n",
        "vector = FAISS.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This creates a specialized chain that can answer questions using specific documents as context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sb63NBGGwuy"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "# create_stuff_documents_chain :\n",
        "#  Create a chain for passing a list of Documents to a model.\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\", output_parser = output_parser)\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "# document_chain = prompt | llm /"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Think of it like a research assistant:\n",
        "\n",
        "- Before: AI answers from its general knowledge\n",
        "- Now: AI gets a stack of specific research papers and must answer ONLY using those papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPdi0y7ZG29G"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = vector.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This line runs the complete RAG system you've been building throughout the notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fchYCGjKHDKu",
        "outputId": "cad3a6a8-d971-433c-8a6b-48f12cb2a28b"
      },
      "outputs": [],
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmP7v_s7HHmv",
        "outputId": "e06297e0-8ff8-4fa3-a02c-c163487b44fe"
      },
      "outputs": [],
      "source": [
        "print(response[\"answer\"])\n",
        "\n",
        "# LangSmith offers several features that can help with testing:..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYwphfNwYefc",
        "outputId": "e51ad310-453a-4f21-e658-b6361a554aa0"
      },
      "outputs": [],
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"how can use it?\"})\n",
        "print(response[\"answer\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
